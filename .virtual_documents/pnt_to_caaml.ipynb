


import sys
sys.path.append('D:\\SMP_stability\\snowmicropyn')
import snowmicropyn as smpyn
from snowmicropyn.serialize import caaml
from snowmicropyn.ai.grain_classifier import grain_classifier
import pathlib
import numpy as np
from matplotlib import pyplot as plt
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler





#prof = Profile.load('snowmicropyn/examples/profiles/S37M0876.pnt')
prof = smpyn.Profile.load('.pnt_data\\190110_BUN\\SMP\\FILE0245.pnt') 


%matplotlib inline


# Plot distance on x and samples on y axis
plt.plot(prof.samples.distance, prof.samples.force)

# Prettify our plot a bit
plt.title(prof.name)
plt.ylabel('Force [N]')
plt.xlabel('Depth [mm]')
plt.ylim(-0.0,1)
plt.xlim(670,685)
# Show interactive plot with zoom, export and other features
plt.show()





0.2*0.007








0.5*120 *9.81*0.007

















samples = prof.samples_within_snowpack()
parameterization = "CR2020"
# Prebpare derivatives:
param = smpyn.derivatives.parameterizations[parameterization]
#param.overlap = 50
param.window_size = 2
loewe2012_df = smpyn.loewe2012.calc(samples, param.window_size, param.overlap)
derivatives = loewe2012_df
derivatives = derivatives.merge(param.calc_from_loewe2012(loewe2012_df))
derivatives.profile_bottom = derivatives.distance.iloc[-1]








derivatives.keys()


# Combine absolute values and derivatives into a feature matrix
features = np.column_stack((derivatives.force_median.values ,
                            derivatives.L2012_f0.values ,
                            derivatives.L2012_delta.values, 
                            derivatives.L2012_L.values, 
                            derivatives.CR2020_ssa.values, 
                            derivatives.CR2020_density.values))
x = derivatives.distance.values

weights = np.array([3,1,1,3,3,1])  # feature weighting, more weight on force, density and SSA
scaler = StandardScaler()
scaled_features = scaler.fit_transform(features)
scaled_features = scaled_features * weights





import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs
from scipy.spatial.distance import euclidean
from scipy.spatial import ConvexHull
import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
plt.figure()

# Calculate WCSS for different numbers of clusters
wcss = []
min_cluster,max_cluster = 1, 20
cluster_range = np.arange(1,20)
for k in cluster_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(scaled_features)
    wcss.append(kmeans.inertia_)

# Straight line connecting endpoints
line_start = np.array([cluster_range[0], wcss[0]])
line_end = np.array([cluster_range[-1], wcss[-1]])
# Calculate distances from each point to the line
distances = []
for i in range(len(cluster_range)):
    point = np.array([cluster_range[i], wcss[i]])
    distance = np.abs(np.cross(line_end-line_start, line_start-point)) / np.linalg.norm(line_end-line_start)
    distances.append(distance)

# Find the point of maximum distance
kink_index = np.argmax(distances)
kink_x = cluster_range[kink_index]
kink_y = wcss[kink_index]
# Plot the Elbow Method
plt.plot(cluster_range, wcss, marker='o')
plt.plot(kink_x,kink_y,marker="o",c="r")
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS')
plt.title('Elbow Method')
plt.show()

opt_number_clusters = kink_x
print("optimal_number_clusters: " + str(opt_number_clusters))


def norm(df):
    return((df - df.min()) / (df.max() - df.min()))

# Use KMeans to cluster based on both features
additional_clusters = 0 # weak layers are sometimes thin and just little weaker as the rest of the layer so we allow more clusters than "necessary"
n_clusters = opt_number_clusters + additional_clusters # Adjust the number of blocks
kmeans = KMeans(n_clusters=n_clusters, random_state=42, )
labels = kmeans.fit_predict(scaled_features)
derivatives["cluster_id"] = labels

# Find boundaries based on clusters
boundaries = [x[0]]  # Start with the first point
for i in range(1, len(labels)):
    if labels[i] != labels[i - 1]:  # If the cluster label changes
        boundaries.append(x[i])
boundaries.append(x[-1])  # Add the last point

# Compute average values in each block
block_values = []
for i in range(len(boundaries) - 1):
    start, end = boundaries[i], boundaries[i + 1]
    mask = (x >= start) & (x < end)
    block_avg = np.mean(derivatives.CR2020_density[mask])
    block_values.append(block_avg)

%matplotlib inline
plt.figure(figsize=(10, 6))
plt.plot(x, norm(derivatives.CR2020_density), label="CR2020_density", color="blue")
plt.plot(x, norm(derivatives.CR2020_ssa), label="CR2020_ssa", color="k")
plt.plot(x, norm(derivatives.force_median), label="force_median", color="red")
plt.legend()

for i in range(len(boundaries) - 1):
    col = f"C{int(derivatives.loc[derivatives['distance'] == boundaries[i], 'cluster_id'].iloc[0])}"
    plt.axvspan(boundaries[i], boundaries[i + 1], color=col, alpha=0.3, label=f"Block {i+1}")
plt.vlines(prof.markers["wl_t"]-prof.markers["surface"],0,1,colors="k")
plt.vlines(prof.markers["wl_b"]-prof.markers["surface"],0,1,colors="k")
plt.show()





derivatives['layer_id'] = (derivatives['cluster_id'] != derivatives['cluster_id'].shift()).cumsum()





layered_derivatives = derivatives.groupby('layer_id').mean()
layered_derivatives["distance"] = derivatives.groupby('layer_id').min().distance # distance is the lower boundary in the snowpack, 0 is surface
layered_derivatives.profile_bottom = derivatives.profile_bottom
layered_derivatives.index = layered_derivatives.index - 1
layered_derivatives = layered_derivatives.rename(columns={'distance': 'depthTop'})
layered_derivatives.profile_bottom = derivatives.profile_bottom


thickness_values = []
for idx, row in layered_derivatives.iterrows():
    if idx == len(layered_derivatives) - 1:
        layer_thickness = layered_derivatives.profile_bottom - row.depthTop
        thickness_values.append(layer_thickness)
    else:
        layer_thickness = layered_derivatives.depthTop[idx + 1] - row.depthTop
        thickness_values.append(layer_thickness)
layered_derivatives['thickness'] = thickness_values
layered_derivatives = layered_derivatives[['depthTop','thickness', 'force_median', 'L2012_lambda', 'L2012_f0',
         'L2012_delta', 'L2012_L', 'CR2020_density', 'CR2020_ssa', 'cluster_id']] # Reorder the DataFrame columns





derivatives.to_pickle(prof._pnt_file.with_name(f'{prof._pnt_file.stem}_smp_ful').with_suffix('.pkl'))
layered_derivatives.to_pickle(prof._pnt_file.with_name(f'{prof._pnt_file.stem}_smp_lay').with_suffix('.pkl'))


























--> caaml exporting will not work anymore as i deleted the column "distance", also caaml.py is affected

from snowmicropyn.serialize import caaml
from snowmicropyn.ai.grain_classifier import grain_classifier

outfile = None
export_settings = {'export_grainshape':True, 'use_pretrained_model': True, 'trained_input_path': 'snowmicropyn/snowmicropyn/ai/trained_model_rhossa.dat'}
prof._altitude = 1600
df = layered_derivatives

# add _smp flag to file name in order to (hopefully) not overwrite hand profiles:
stem = f'{prof._pnt_file.stem}_smp'
if outfile:
    outfile = pathlib.Path(outfile) # full file name was given
    if outfile.is_dir(): # folder name was given -> choose filename
        outfile = pathlib.Path(f'{outfile}/{stem}.caaml')
else: # no name was given --> choose full path
    outfile = prof._pnt_file.with_name(stem).with_suffix('.caaml')

grain_shapes = {}
if export_settings.get('export_grainshape', False): # start machine learning process
    classifier = grain_classifier(export_settings)
    grain_shapes = classifier.predict(df.drop(["CR2020_density","CR2020_ssa","cluster_id"],axis=1))

caaml.export(export_settings, df, grain_shapes,
    prof._pnt_file.stem, prof._timestamp, prof._smp_serial,
    prof._longitude, prof._latitude, prof._altitude, outfile)




